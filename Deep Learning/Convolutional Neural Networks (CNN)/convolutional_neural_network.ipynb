{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3DR-eO17geWu"
   },
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "**steps:**\n",
    "1. Convolution\n",
    "2. Max Pooling\n",
    "3. Flattening\n",
    "4. Full Connnection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution\n",
    "The formula is:\n",
    "\n",
    "$$(f*g)(t) = \\int^\\infty_{-\\infty} f(\\tau)g(t- \\tau) d\\tau$$\n",
    "\n",
    "Feature detectors(Kenel or filter are other names) are usually 3 by 3(There are reason for that. There are other versions like 7 by 7\n",
    "\n",
    "Convolutional operation is show by x in a circle $\\otimes$\n",
    "\n",
    "\n",
    "Multiplying each value by each value from input image to feature detector(elementwise) and then you add up the result. Result is not binary but integer\n",
    "\n",
    "Step at which we are moving is called stride\n",
    "\n",
    "The result would be called a feature map ( activation map our convolved feature are other names)\n",
    "\n",
    "The size of stride reduces the size of image --> easier to process and faster but we are losing some information \n",
    "\n",
    "The highest number you can get is when the feature mapps exactly. \n",
    "\n",
    "We create many feature maps that create our first convolution layer\n",
    "\n",
    "There are convolution matrix calculator \n",
    "\n",
    "You can sharpen, blur, edge enhance, edge detect "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: ReLU (REctified Linear Unit):\n",
    "\n",
    "The reason to apply rectifier is to increase non linarity in our image or our neural network. The reason is images themselves are highly non linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: MAX pooling \n",
    "Make suer our neuarl network has spatial invariant: It does not care if the features are a little bit tilted, features are a little apart etc. In short, to have some level of flexibility to find that feature. \n",
    "\n",
    "We also have Min pooling, sum pooling etc\n",
    "\n",
    "Example: From the feature map(eg 5*5) take a box of (eg 2*2), find the max value from that box and put it in the pooled feature map(eg 3*3) --> with stride of two. The 2*2 window and get outside of the feature map (both horizontally and vertically)\n",
    "\n",
    "Benefit of pooling: Reducing the size and improving the preformance of computation. Also, perventing the overfitting \n",
    "\n",
    "Read the relevant paper it takes 20 min \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Flattening\n",
    "The result of flattening would be our input layer for the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: Full Connection\n",
    "input layer -> fully connected layer -> output layer\n",
    "* 1 means that the neuron was very sure that found a certain feature \n",
    "* 0 means that the neuron did not find any feature \n",
    "* sample is a row in dataset\n",
    "* epoch is when you go through the whole dataset again and again \n",
    "* The NN(output neurons) learns to which of the final neurons in the fully connected layer it should listen. That's how features are propagated through network and conveyed to the output\n",
    "* In bach propagation process we adjust the feature detectors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EMefrVPCg-60"
   },
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sCV30xyVhFbE"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FIleuCAjoFD8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.15.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oxQxCBWyoGPE"
   },
   "source": [
    "## Part 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MvE-heJNo3GG"
   },
   "source": [
    "### Preprocessing the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0koUcJMJpEBD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# we augment the imgaes to prevent overfitting\n",
    "# Check keras website for more\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "training_set = train_datagen.flow_from_directory('/Users/jaber/downloads/dataset/training_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mrCMmGw9pHys"
   },
   "source": [
    "### Preprocessing the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SH4WzfOhpKc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# we do not augment the test dataset but need to rescale it. \n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_set = test_datagen.flow_from_directory('/Users/jaber/downloads/dataset/test_set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "af8O4l90gk7B"
   },
   "source": [
    "## Part 2 - Building the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ces1gXY2lmoX"
   },
   "source": [
    "### Initialising the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SAUt4UMPlhLS"
   },
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u5YJj_XMl5LF"
   },
   "source": [
    "### Step 1 - Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XPzPrMckl-hV"
   },
   "outputs": [],
   "source": [
    "# Filters: number of feature detectors, kernel_size: size of the feature detector, here it would be 3*3 matrix\n",
    "# input_shape: we resized our images to 64*64 and 3 is for the colored(RGB)\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tf87FpvxmNOJ"
   },
   "source": [
    "### Step 2 - Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ncpqPl69mOac"
   },
   "outputs": [],
   "source": [
    "# pool size  would be 2*2\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xaTOgD8rm4mU"
   },
   "source": [
    "### Adding a second convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i_-FZjn_m8gk"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tmiEuvTunKfk"
   },
   "source": [
    "### Step 3 - Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6AZeOGCvnNZn"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dAoSECOm203v"
   },
   "source": [
    "### Step 4 - Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8GtmUlLd26Nq"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yTldFvbX28Na"
   },
   "source": [
    "### Step 5 - Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1p_Zj1Mc3Ko_"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D6XkI90snSDl"
   },
   "source": [
    "## Part 3 - Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vfrFQACEnc6i"
   },
   "source": [
    "### Compiling the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NALksrNQpUlJ"
   },
   "outputs": [],
   "source": [
    "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ehS-v3MIpX2h"
   },
   "source": [
    "### Training the CNN on the Training set and evaluating it on the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XUj1W4PJptta"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 42s 163ms/step - loss: 0.6776 - accuracy: 0.6134 - val_loss: 0.6854 - val_accuracy: 0.5860\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 43s 170ms/step - loss: 0.6055 - accuracy: 0.6685 - val_loss: 0.5990 - val_accuracy: 0.6775\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 0.5807 - accuracy: 0.6966 - val_loss: 0.5892 - val_accuracy: 0.6880\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 44s 177ms/step - loss: 0.5545 - accuracy: 0.7156 - val_loss: 0.5693 - val_accuracy: 0.7115\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 38s 151ms/step - loss: 0.5482 - accuracy: 0.7186 - val_loss: 0.5861 - val_accuracy: 0.7040\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 40s 161ms/step - loss: 0.5357 - accuracy: 0.7286 - val_loss: 0.5760 - val_accuracy: 0.7040\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 0.5125 - accuracy: 0.7496 - val_loss: 0.5908 - val_accuracy: 0.7200\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 45s 179ms/step - loss: 0.4947 - accuracy: 0.7545 - val_loss: 0.6803 - val_accuracy: 0.6690\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 0.4893 - accuracy: 0.7648 - val_loss: 0.5983 - val_accuracy: 0.7165\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 55s 221ms/step - loss: 0.4775 - accuracy: 0.7766 - val_loss: 0.5831 - val_accuracy: 0.7250\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 58s 233ms/step - loss: 0.4582 - accuracy: 0.7865 - val_loss: 0.6007 - val_accuracy: 0.7160\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 62s 248ms/step - loss: 0.4540 - accuracy: 0.7794 - val_loss: 0.5756 - val_accuracy: 0.7185\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 64s 256ms/step - loss: 0.4371 - accuracy: 0.7997 - val_loss: 0.5668 - val_accuracy: 0.7340\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 0.4182 - accuracy: 0.8120 - val_loss: 0.5849 - val_accuracy: 0.7300\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 65s 260ms/step - loss: 0.4127 - accuracy: 0.8112 - val_loss: 0.6263 - val_accuracy: 0.7095\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 68s 271ms/step - loss: 0.3980 - accuracy: 0.8174 - val_loss: 0.6195 - val_accuracy: 0.7175\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 67s 268ms/step - loss: 0.3845 - accuracy: 0.8286 - val_loss: 0.6260 - val_accuracy: 0.7290\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 59s 237ms/step - loss: 0.3710 - accuracy: 0.8325 - val_loss: 0.6389 - val_accuracy: 0.7425\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 53s 214ms/step - loss: 0.3599 - accuracy: 0.8394 - val_loss: 0.6121 - val_accuracy: 0.7350\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 57s 226ms/step - loss: 0.3531 - accuracy: 0.8461 - val_loss: 0.6794 - val_accuracy: 0.7260\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 53s 212ms/step - loss: 0.3433 - accuracy: 0.8491 - val_loss: 0.6526 - val_accuracy: 0.7385\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 54s 216ms/step - loss: 0.3240 - accuracy: 0.8590 - val_loss: 0.6698 - val_accuracy: 0.7200\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 60s 240ms/step - loss: 0.3245 - accuracy: 0.8608 - val_loss: 0.6943 - val_accuracy: 0.7295\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 52s 206ms/step - loss: 0.3251 - accuracy: 0.8585 - val_loss: 0.6820 - val_accuracy: 0.7305\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 53s 212ms/step - loss: 0.3096 - accuracy: 0.8677 - val_loss: 0.6865 - val_accuracy: 0.7430\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x130ed1cd0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x = training_set, validation_data = test_set, epochs = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U3PZasO0006Z"
   },
   "source": [
    "## Part 4 - Making a single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gsSiWEJY1BPB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step\n",
      "{'cats': 0, 'dogs': 1}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('/Users/jaber/downloads/dataset/single_prediction/cat_or_dog_1.jpg', target_size = (64, 64))\n",
    "# The predict method requires a 2D array as input \n",
    "test_image = image.img_to_array(test_image)\n",
    "# Cause in the training, we used batches of images, we need to add an extra dimention of batch \n",
    "# The dimention of batch is in the first dimention, so we use axis =0\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = cnn.predict(test_image)\n",
    "print(training_set.class_indices)\n",
    "# we first access the batch and then access the single element of the batch. \n",
    "if result[0][0] == 1:\n",
    "  prediction = 'dog'\n",
    "else:\n",
    "  prediction = 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ED9KB3I54c1i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyON0YxX/oky4tPbqCLnFjWD",
   "collapsed_sections": [],
   "name": "convolutional_neural_network.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
